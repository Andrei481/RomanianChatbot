{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "Xf2a-UMwiBs1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37Uw3BpF2DLS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.current_device())\n",
        "print(torch.cuda.device_count())"
      ],
      "metadata": {
        "id": "5k2UIUXj3XYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting maximum sequence length and quanitzation"
      ],
      "metadata": {
        "id": "ijd18rhCikZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # max we could fit on 1 GPU\n",
        "dtype = float\n",
        "load_in_4bit = True # 4bit quantization"
      ],
      "metadata": {
        "id": "M_U7wt6Q3Xt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load base model and tokenizer"
      ],
      "metadata": {
        "id": "07-MsWmwi5sJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Andrei481/llama3-8b-corpus-ro-8k-16b\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = \"\"\n",
        ")"
      ],
      "metadata": {
        "id": "rQE5lm-KfAWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add LoRA adapters"
      ],
      "metadata": {
        "id": "IoZdp6PYi8kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None\n",
        ")"
      ],
      "metadata": {
        "id": "iUD4MkO-fRHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "9YEZlVJ7jMmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Mai jos este prezentată o instrucțiune care descrie o sarcină, însoțită de o intrare care oferă un context suplimentar. Scrieți un răspuns care să completeze în mod corespunzător cererea. Dacă nu știți răspunsul la o întrebare, vă rugăm să nu împărtășiți informaţii false. Trebuie să răspundeți doar în limba română.\n",
        "\n",
        "### Instrucțiune:\n",
        "{}\n",
        "\n",
        "### Intrare:\n",
        "{}\n",
        "\n",
        "### Răspuns:\n",
        "{}\"\"\"\n",
        "\n",
        "llama3_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "Sunteți un asistent util, respectuos și onest. Dacă o întrebare nu are niciun sens sau nu este coerentă din punct de vedere factual, explicați de ce în loc să răspundeți la ceva incorect. Dacă nu știți răspunsul la o întrebare, vă rugăm să nu împărtășiți informaţii false. Trebuie sa răspundeți doar în limba română.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "def format_prompt_chat_llama3(data):\n",
        "    instructions = data[\"instruction\"]\n",
        "    inputs = data[\"input\"]\n",
        "    outputs = data[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        if input is not None:\n",
        "            text = llama3_prompt.format(instruction + \" \" + input, output) + EOS_TOKEN # IMPORTANT - WITHOUT THIS, GENERATION WILL GO ON FOREVER\n",
        "        else:\n",
        "            text = llama3_prompt.format(instruction, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts }"
      ],
      "metadata": {
        "id": "eXcgD-QpfUpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token\n",
        "BOS_TOKEN = tokenizer.bos_token\n",
        "PAD_TOKEN = tokenizer.pad_token\n",
        "print(EOS_TOKEN)\n",
        "print(BOS_TOKEN)\n",
        "print(PAD_TOKEN)"
      ],
      "metadata": {
        "id": "M_GWO_c7fYaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(format_prompt_chat_llama3, batched=True)"
      ],
      "metadata": {
        "id": "e7EoBWPkgczK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"text\"][5]"
      ],
      "metadata": {
        "id": "OusJcdzygfQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "PtZb162HjSQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "lCseu7lFggqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "DP9FI7Osgsqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "wjx-jGPQjqKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    llama3_prompt.format(\n",
        "        \"Care sunt condițiile necesare pentru a avea un deadlock în programare?\", # instruction\n",
        "        \"\", # input (optional)\n",
        "        \"\", # output (leave empty)\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "dg7FePvIhOMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save finetuned model locally"
      ],
      "metadata": {
        "id": "n5nNMvlfjsZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"llama-3-8b-unsloth-corpus-hakurei-ro-lora-adapters\")"
      ],
      "metadata": {
        "id": "C2q8L8SRhP_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save finetuned to HuggingFace"
      ],
      "metadata": {
        "id": "IPmcY5iDjuac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if True: model.save_pretrained_merged(\"llama-3-8b-unsloth-corpus-hakurei-ro-16b\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if True: model.push_to_hub_merged(\"Andrei481/llama-3-8b-unsloth-corpus-hakurei-ro-16b\", tokenizer, save_method = \"merged_16bit\", token = \"\", private=True)"
      ],
      "metadata": {
        "id": "MteNkKTEhgrn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}